import numpy as np

class Module(object):
    def __init__(self):
        self._parameters = None
        self._gradient = None

    def zero_grad(self):
        ## Annule gradient
        pass

    def forward(self, X):
        ## Calcule la passe forward
        pass

    def update_parameters(self, gradient_step=1e-3):
        ## Calcule la mise a jour des parametres selon le gradient calcule et le pas de gradient_step
        self._parameters -= gradient_step*self._gradient

    def backward_update_gradient(self, input, delta):
        ## Met a jour la valeur du gradient
        pass

    def backward_delta(self, input, delta):
        ## Calcul la derivee de l'erreur
        pass

class Linear(Module):    
    def __init__(self, dimensions = None, init = 'xavier', bias = True):
        """
        Dimensions : un tuple (dim_in, dim_out), si les dimensions sont passées
        initialise les parameters aléatoirement selon la méthode définit dans init,
        uniforme sinon.
        bias : if true, ajoute un bias au module
        
        """
        
        def initialise(dimensions, type_):
            if type(dimensions) == type(None) or type(type_) == type(None):
                return None
            if type_ == "randn":
                return np.random.randn(dimensions[0], dimensions[1]) -0.5
            if type_ == 'xavier':
                return (np.random.randn(dimensions[0], dimensions[1]) -0.5)*np.sqrt(2/sum(dimensions))
            if type_ == 'xavier_tanh':
                return (np.random.randn(dimensions[0], dimensions[1]) -0.5)*np.sqrt(2)*np.sqrt(2/sum(dimensions))
            if type_ == 'uniform':
                return (np.random.random(dimensions) -0.5)
            raise Exception('initialisation inconnue')            
        
        self._gradient   = None
        self._bias = None
        self._parameters = initialise(dimensions, init)
        if type(self.parameters) != type(None):
            if bias:
                self._bias = initialise((1,dimensions[1]), init)
            else:
                
    
    def forward(self, X):
        """
        input  : batch * input
        output : batch * output
        """
        assert len(X.shape) == 2
        tmp = np.dot(X, self._parameters)
        if type(self._bias) != type(None):
            return tmp + self._bias
        return tmp
    
    def update_parameters(self, gradient_step=1e-3):
        ## Calcule la mise a jour des parametres selon le gradient calcule et le pas de gradient_step
        self._parameters -= gradient_step*self._gradient
        
        if type(self._bias) != type(None):
            self._bias -=gradient_step*self._biasgrad
    
    def backward_update_gradient(self, input, delta):
        ## Met a jour la valeur du gradient
        try:
            self._gradient += np.dot(input.T, delta)
        except (AttributeError, TypeError): # Si _gradient n'existe pas, ou si il vaut None
            self._gradient = np.dot(input.T, delta)
        
        if type(self._bias) != type(None):
            try:
                self._biasgrad += np.sum(delta, axis = 0)
            except (AttributeError, TypeError):
                self._biasgrad = np.sum(delta, axis = 0)
                
    def backward_delta(self, input, delta):
        #Doit avoir la même dimension que l'inputS
        return np.dot(delta, self._parameters.T)
    
    def zero_grad(self):
        self._biasgrad = None
        self._gradient = None
    
class TanH(Module):
    def forward(self, X):
        ## Calcule la passe forward
        return np.tanh(X)
    
    def backward_delta(self, input, delta):
        ## Calcul la derivee de l'erreur
        return delta * (1-np.power(np.tanh(input),2))
    
    def update_parameters(self, gradient_step=1e-3):
        pass
        
class Sigmoid(Module):
    def forward(self, X):
        ## Calcule la passe forward
        return 1/(1 + np.exp(-X))

    def backward_delta(self, input, delta):
        ## Calcul la derivee de l'erreur
        tmp = np.exp(-input)
        return delta * (tmp/np.power(1+tmp, 2))
    
    def update_parameters(self, gradient_step=1e-3):
        pass

class Softmax(Module):
    def forward(self, X):
        ## Calcule la passe forward
        exp = np.exp(X)
        return exp/np.expand_dims(np.sum(exp ,axis = 1), axis = 1)

    def backward_delta(self, input, delta):
        ## Calcul la derivee de l'erreur
        exp = np.exp(input)
        tmp = exp/np.expand_dims(np.sum(exp ,axis = 1), axis = 1)
        return delta *tmp*(1-tmp)
    
    def update_parameters(self, gradient_step=1e-3):
        pass
